{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import re\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r\"C:\\Users\\SPECTRE\\Documents\\ML\\projects\\Cats and Dogs\"\n",
    "train_dir = os.path.join(dir, 'images_train')\n",
    "test_dir = os.path.join(dir, 'images_test')\n",
    "train_csv = os.path.join(dir, 'train.csv')\n",
    "test_csv = os.path.join(dir, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SPECTRE\\\\Documents\\\\ML\\\\projects\\\\Cats and Dogs\\\\train.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pandas.read_csv(train_csv)\n",
    "testData = pandas.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_0966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_6542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_0763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_6579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id\n",
       "0  img_0966\n",
       "1  img_6542\n",
       "2  img_2360\n",
       "3  img_0763\n",
       "4  img_6579"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainData['breed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = LabelBinarizer().fit(m)\n",
    "x = en.transform(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "newBreed = en.inverse_transform(newBreed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14, 21, ..., 23, 14, 15])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newBreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabels = 2\n",
    "breedLabels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(64, 64, 3))\n",
    "conv1 = keras.layers.Conv2D(64, 5, 1, activation=tf.nn.relu, data_format = 'channels_last')(inp)\n",
    "conv2 = keras.layers.Conv2D(64,5, 1, activation=tf.nn.relu)(conv1)\n",
    "max1 = keras.layers.MaxPool2D(pool_size = (2,2),strides= 2, padding= \"same\")(conv2)\n",
    "norm1 = keras.layers.BatchNormalization()(max1)\n",
    "conv3 = keras.layers.Conv2D(64,3, 1, activation=tf.nn.relu)(norm1)\n",
    "conv4 = keras.layers.Conv2D(64,3, 1, activation=tf.nn.relu)(conv3)\n",
    "max2 = keras.layers.MaxPool2D(pool_size = (2,2),strides= 2, padding= \"same\")(conv4)\n",
    "norm2 = keras.layers.BatchNormalization()(max2)\n",
    "conv5 = keras.layers.Conv2D(128,3, 1, activation=tf.nn.selu)(norm2)\n",
    "conv6 = keras.layers.Conv2D(128,3, 1, activation=tf.nn.selu)(conv5)\n",
    "avg1 = keras.layers.AvgPool2D(pool_size = (2,2),strides= 2, padding= \"same\")(conv6)\n",
    "flat = keras.layers.Flatten()(avg1)\n",
    "fc1 = keras.layers.Dense(1024, activation=tf.nn.relu)(flat)\n",
    "fc1 = keras.layers.Dropout(0.2)(fc1)\n",
    "fc2 = keras.layers.Dense(512, activation=tf.nn.relu)(fc1)\n",
    "fc2 = keras.layers.Dropout(0.2)(fc2)\n",
    "fc3 = keras.layers.Dense(256, activation=tf.nn.relu)(fc2)\n",
    "fc3 = keras.layers.Dropout(0.2)(fc3)\n",
    "fc4 = keras.layers.Dense(128, activation=tf.nn.relu)(fc3)\n",
    "fc4 = keras.layers.Dropout(0.3)(fc4)\n",
    "\n",
    "classLabel = keras.layers.Dense(classLabels, activation=tf.nn.softmax, name='class')(fc2)\n",
    "breedLabel = keras.layers.Dense(breedLabels, activation=tf.nn.softmax, name='breed')(fc4)\n",
    "\n",
    "model1 = keras.Model(inputs = inp, outputs=[classLabel, breedLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 60, 60, 64)   4864        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 56, 56, 64)   102464      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 28, 28, 64)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 64)   256         max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 26, 26, 64)   36928       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 24, 24, 64)   36928       conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 12, 12, 64)   0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 12, 12, 64)   256         max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 10, 10, 128)  73856       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 128)    147584      conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 4, 4, 128)    0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1024)         2098176     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1024)         0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 512)          524800      dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 512)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256)          131328      dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 256)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          32896       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "class (Dense)                   (None, 2)            1026        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "breed (Dense)                   (None, 10)           1290        dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,192,652\n",
      "Trainable params: 3,192,396\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='sgd', loss = {'class':\"categorical_crossentropy\", 'breed':'categorical_crossentropy'}, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(imagePath, size):\n",
    "    return img_to_array(load_img(imagePath, target_size=(size, size))) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSequence(keras.utils.Sequence):\n",
    "    def __init__(self, df_path, data_path, im_size, batch_size, mode='train'):\n",
    "        self.df = pandas.read_csv(df_path)\n",
    "        self.im_size = im_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.imagePathList = self.df['id'].apply(lambda x: os.path.join(data_path, x + '.jpg')).tolist()\n",
    "        if self.mode == 'train':\n",
    "            self.classLabelList = self.df['class_name'].apply(lambda x: float(x)).tolist()\n",
    "            self.breedLabelList = self.df['breed'].apply(lambda x: float(x)).tolist()\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.onehot_encoder = OneHotEncoder(sparse=False)\n",
    "            self.integer_encoded = self.label_encoder.fit_transform(self.classLabelList)\n",
    "            self.integer_encoded = self.integer_encoded.reshape(len(self.integer_encoded), 1)\n",
    "            self.classLabelEnc = self.onehot_encoder.fit_transform(self.integer_encoded)\n",
    "            self.integer1_encoded = self.label_encoder.fit_transform(self.breedLabelList)\n",
    "            self.integer1_encoded = self.integer1_encoded.reshape(len(self.integer1_encoded), 1)\n",
    "            self.breedLabelEnc = self.onehot_encoder.fit_transform(self.integer1_encoded)\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = range(len(self.imagePathList))\n",
    "        if self.mode == 'train':\n",
    "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
    "            \n",
    "    def get_batch_labels(self, idx): \n",
    "        # Fetch a batch of labels\n",
    "        return [self.classLabelEnc[idx * self.batch_size: (idx + 1) * self.batch_size],\n",
    "                self.breedLabelEnc[idx * self.batch_size: (idx + 1) * self.batch_size]]\n",
    "\n",
    "    def get_batch_features(self, idx):\n",
    "        # Fetch a batch of images\n",
    "        batch_images = self.imagePathList[idx * self.batch_size: (1 + idx) * self.batch_size]\n",
    "        return np.array([load_image(im, self.im_size) for im in batch_images])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            batch_x = self.get_batch_features(idx)\n",
    "            batch_y = self.get_batch_labels(idx)\n",
    "            return batch_x, batch_y\n",
    "        else:\n",
    "            batch_x = self.get_batch_features(idx)\n",
    "            return batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SPECTRE\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\SPECTRE\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "obj = ImageSequence(train_csv, train_dir, im_size = 64, batch_size=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:ModelCheckpoint mode <built-in function min> is unknown, fallback to auto mode.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('./model.h1', verbose=1, save_best_only=True, mode=min)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 2.8639 - class_loss: 0.6091 - breed_loss: 2.2548 - class_acc: 0.6813 - breed_acc: 0.1607WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 593s 4s/step - loss: 2.8637 - class_loss: 0.6088 - breed_loss: 2.2548 - class_acc: 0.6813 - breed_acc: 0.1608\n",
      "Epoch 2/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 2.5467 - class_loss: 0.5103 - breed_loss: 2.0365 - class_acc: 0.7584 - breed_acc: 0.2526WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 613s 4s/step - loss: 2.5470 - class_loss: 0.5106 - breed_loss: 2.0364 - class_acc: 0.7577 - breed_acc: 0.2529\n",
      "Epoch 3/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 2.3580 - class_loss: 0.4632 - breed_loss: 1.8948 - class_acc: 0.7851 - breed_acc: 0.3097WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 615s 4s/step - loss: 2.3582 - class_loss: 0.4633 - breed_loss: 1.8949 - class_acc: 0.7846 - breed_acc: 0.3096\n",
      "Epoch 4/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 2.2169 - class_loss: 0.4179 - breed_loss: 1.7990 - class_acc: 0.8117 - breed_acc: 0.3466WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 2.2199 - class_loss: 0.4191 - breed_loss: 1.8008 - class_acc: 0.8111 - breed_acc: 0.3462\n",
      "Epoch 5/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 2.0655 - class_loss: 0.3774 - breed_loss: 1.6881 - class_acc: 0.8313 - breed_acc: 0.3958WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 2.0638 - class_loss: 0.3770 - breed_loss: 1.6868 - class_acc: 0.8316 - breed_acc: 0.3965\n",
      "Epoch 6/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.9589 - class_loss: 0.3488 - breed_loss: 1.6101 - class_acc: 0.8464 - breed_acc: 0.4198WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 1.9568 - class_loss: 0.3485 - breed_loss: 1.6083 - class_acc: 0.8464 - breed_acc: 0.4205\n",
      "Epoch 7/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.8706 - class_loss: 0.3205 - breed_loss: 1.5501 - class_acc: 0.8627 - breed_acc: 0.4378WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 617s 4s/step - loss: 1.8708 - class_loss: 0.3213 - breed_loss: 1.5494 - class_acc: 0.8625 - breed_acc: 0.4383\n",
      "Epoch 8/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.7683 - class_loss: 0.2955 - breed_loss: 1.4728 - class_acc: 0.8748 - breed_acc: 0.4661WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 1.7659 - class_loss: 0.2945 - breed_loss: 1.4713 - class_acc: 0.8756 - breed_acc: 0.4660\n",
      "Epoch 9/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.6997 - class_loss: 0.2730 - breed_loss: 1.4267 - class_acc: 0.8881 - breed_acc: 0.4899WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 615s 4s/step - loss: 1.7006 - class_loss: 0.2737 - breed_loss: 1.4269 - class_acc: 0.8881 - breed_acc: 0.4900\n",
      "Epoch 10/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.6090 - class_loss: 0.2504 - breed_loss: 1.3586 - class_acc: 0.8958 - breed_acc: 0.5258WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 1.6091 - class_loss: 0.2504 - breed_loss: 1.3587 - class_acc: 0.8959 - breed_acc: 0.5250\n",
      "Epoch 11/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.5227 - class_loss: 0.2248 - breed_loss: 1.2979 - class_acc: 0.9067 - breed_acc: 0.5362WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 616s 4s/step - loss: 1.5226 - class_loss: 0.2240 - breed_loss: 1.2986 - class_acc: 0.9072 - breed_acc: 0.5361\n",
      "Epoch 12/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.4321 - class_loss: 0.2000 - breed_loss: 1.2321 - class_acc: 0.9183 - breed_acc: 0.5544WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 606s 4s/step - loss: 1.4315 - class_loss: 0.1995 - breed_loss: 1.2321 - class_acc: 0.9186 - breed_acc: 0.5540\n",
      "Epoch 13/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.3277 - class_loss: 0.1724 - breed_loss: 1.1553 - class_acc: 0.9308 - breed_acc: 0.5903WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 623s 4s/step - loss: 1.3287 - class_loss: 0.1727 - breed_loss: 1.1560 - class_acc: 0.9310 - breed_acc: 0.5901\n",
      "Epoch 14/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.2537 - class_loss: 0.1510 - breed_loss: 1.1027 - class_acc: 0.9416 - breed_acc: 0.5992WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 619s 4s/step - loss: 1.2520 - class_loss: 0.1512 - breed_loss: 1.1008 - class_acc: 0.9413 - breed_acc: 0.6001\n",
      "Epoch 15/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.1853 - class_loss: 0.1364 - breed_loss: 1.0489 - class_acc: 0.9464 - breed_acc: 0.6214WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 615s 4s/step - loss: 1.1855 - class_loss: 0.1365 - breed_loss: 1.0490 - class_acc: 0.9463 - breed_acc: 0.6217\n",
      "Epoch 16/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.1061 - class_loss: 0.1169 - breed_loss: 0.9893 - class_acc: 0.9545 - breed_acc: 0.6472WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 618s 4s/step - loss: 1.1049 - class_loss: 0.1167 - breed_loss: 0.9882 - class_acc: 0.9545 - breed_acc: 0.6476\n",
      "Epoch 17/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 1.0390 - class_loss: 0.0982 - breed_loss: 0.9408 - class_acc: 0.9656 - breed_acc: 0.6622WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 618s 4s/step - loss: 1.0402 - class_loss: 0.0984 - breed_loss: 0.9418 - class_acc: 0.9657 - breed_acc: 0.6620\n",
      "Epoch 18/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.9592 - class_loss: 0.0838 - breed_loss: 0.8754 - class_acc: 0.9714 - breed_acc: 0.6818WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 622s 4s/step - loss: 0.9616 - class_loss: 0.0845 - breed_loss: 0.8771 - class_acc: 0.9713 - breed_acc: 0.6810\n",
      "Epoch 19/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.8962 - class_loss: 0.0769 - breed_loss: 0.8193 - class_acc: 0.9722 - breed_acc: 0.6985WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 602s 4s/step - loss: 0.8978 - class_loss: 0.0769 - breed_loss: 0.8209 - class_acc: 0.9721 - breed_acc: 0.6976\n",
      "Epoch 20/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.8418 - class_loss: 0.0574 - breed_loss: 0.7844 - class_acc: 0.9817 - breed_acc: 0.7140WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 615s 4s/step - loss: 0.8409 - class_loss: 0.0573 - breed_loss: 0.7836 - class_acc: 0.9816 - breed_acc: 0.7147\n",
      "Epoch 21/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.7665 - class_loss: 0.0542 - breed_loss: 0.7122 - class_acc: 0.9836 - breed_acc: 0.7394WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 664s 5s/step - loss: 0.7686 - class_loss: 0.0545 - breed_loss: 0.7142 - class_acc: 0.9836 - breed_acc: 0.7389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.7101 - class_loss: 0.0419 - breed_loss: 0.6681 - class_acc: 0.9890 - breed_acc: 0.7580WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 638s 5s/step - loss: 0.7100 - class_loss: 0.0418 - breed_loss: 0.6682 - class_acc: 0.9890 - breed_acc: 0.7585\n",
      "Epoch 23/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.6672 - class_loss: 0.0378 - breed_loss: 0.6294 - class_acc: 0.9878 - breed_acc: 0.7682WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 634s 4s/step - loss: 0.6681 - class_loss: 0.0380 - breed_loss: 0.6301 - class_acc: 0.9876 - breed_acc: 0.7677\n",
      "Epoch 24/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.6627 - class_loss: 0.0494 - breed_loss: 0.6133 - class_acc: 0.9838 - breed_acc: 0.7766WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 634s 4s/step - loss: 0.6608 - class_loss: 0.0492 - breed_loss: 0.6116 - class_acc: 0.9837 - breed_acc: 0.7772\n",
      "Epoch 25/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.5786 - class_loss: 0.0307 - breed_loss: 0.5479 - class_acc: 0.9909 - breed_acc: 0.7984WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 628s 4s/step - loss: 0.5785 - class_loss: 0.0306 - breed_loss: 0.5479 - class_acc: 0.9910 - breed_acc: 0.7983\n",
      "Epoch 26/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.5086 - class_loss: 0.0272 - breed_loss: 0.4814 - class_acc: 0.9914 - breed_acc: 0.8248WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 634s 4s/step - loss: 0.5068 - class_loss: 0.0270 - breed_loss: 0.4798 - class_acc: 0.9915 - breed_acc: 0.8256\n",
      "Epoch 27/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.4655 - class_loss: 0.0202 - breed_loss: 0.4453 - class_acc: 0.9953 - breed_acc: 0.8383WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 655s 5s/step - loss: 0.4639 - class_loss: 0.0202 - breed_loss: 0.4438 - class_acc: 0.9953 - breed_acc: 0.8388\n",
      "Epoch 28/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.4341 - class_loss: 0.0189 - breed_loss: 0.4152 - class_acc: 0.9946 - breed_acc: 0.8505WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 630s 4s/step - loss: 0.4342 - class_loss: 0.0190 - breed_loss: 0.4152 - class_acc: 0.9945 - breed_acc: 0.8504\n",
      "Epoch 29/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.4202 - class_loss: 0.0236 - breed_loss: 0.3966 - class_acc: 0.9930 - breed_acc: 0.8567WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 656s 5s/step - loss: 0.4205 - class_loss: 0.0234 - breed_loss: 0.3971 - class_acc: 0.9931 - breed_acc: 0.8562\n",
      "Epoch 30/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.3936 - class_loss: 0.0226 - breed_loss: 0.3710 - class_acc: 0.9927 - breed_acc: 0.8700WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 668s 5s/step - loss: 0.3938 - class_loss: 0.0226 - breed_loss: 0.3712 - class_acc: 0.9927 - breed_acc: 0.8701\n",
      "Epoch 31/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.3460 - class_loss: 0.0206 - breed_loss: 0.3253 - class_acc: 0.9933 - breed_acc: 0.8833WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 692s 5s/step - loss: 0.3459 - class_loss: 0.0205 - breed_loss: 0.3253 - class_acc: 0.9934 - breed_acc: 0.8833\n",
      "Epoch 32/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.3209 - class_loss: 0.0196 - breed_loss: 0.3013 - class_acc: 0.9945 - breed_acc: 0.8924WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 642s 5s/step - loss: 0.3212 - class_loss: 0.0196 - breed_loss: 0.3016 - class_acc: 0.9945 - breed_acc: 0.8926\n",
      "Epoch 33/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.2641 - class_loss: 0.0105 - breed_loss: 0.2536 - class_acc: 0.9985 - breed_acc: 0.9128WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 652s 5s/step - loss: 0.2635 - class_loss: 0.0105 - breed_loss: 0.2531 - class_acc: 0.9985 - breed_acc: 0.9130\n",
      "Epoch 34/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.2526 - class_loss: 0.0109 - breed_loss: 0.2417 - class_acc: 0.9979 - breed_acc: 0.9164WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 647s 5s/step - loss: 0.2526 - class_loss: 0.0109 - breed_loss: 0.2417 - class_acc: 0.9979 - breed_acc: 0.9162\n",
      "Epoch 35/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.2488 - class_loss: 0.0163 - breed_loss: 0.2326 - class_acc: 0.9963 - breed_acc: 0.9144WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 672s 5s/step - loss: 0.2489 - class_loss: 0.0162 - breed_loss: 0.2327 - class_acc: 0.9963 - breed_acc: 0.9141\n",
      "Epoch 36/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.2148 - class_loss: 0.0131 - breed_loss: 0.2017 - class_acc: 0.9969 - breed_acc: 0.9289WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 673s 5s/step - loss: 0.2156 - class_loss: 0.0130 - breed_loss: 0.2025 - class_acc: 0.9969 - breed_acc: 0.9284\n",
      "Epoch 37/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.2041 - class_loss: 0.0117 - breed_loss: 0.1924 - class_acc: 0.9976 - breed_acc: 0.9336WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 667s 5s/step - loss: 0.2036 - class_loss: 0.0116 - breed_loss: 0.1920 - class_acc: 0.9976 - breed_acc: 0.9338\n",
      "Epoch 38/40\n",
      "140/141 [============================>.] - ETA: 4s - loss: 0.1698 - class_loss: 0.0109 - breed_loss: 0.1589 - class_acc: 0.9979 - breed_acc: 0.9446WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 668s 5s/step - loss: 0.1695 - class_loss: 0.0108 - breed_loss: 0.1587 - class_acc: 0.9979 - breed_acc: 0.9447\n",
      "Epoch 39/40\n",
      "140/141 [============================>.] - ETA: 5s - loss: 0.1696 - class_loss: 0.0097 - breed_loss: 0.1599 - class_acc: 0.9979 - breed_acc: 0.9432 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 736s 5s/step - loss: 0.1697 - class_loss: 0.0097 - breed_loss: 0.1600 - class_acc: 0.9979 - breed_acc: 0.9431\n",
      "Epoch 40/40\n",
      "140/141 [============================>.] - ETA: 5s - loss: 0.1519 - class_loss: 0.0084 - breed_loss: 0.1434 - class_acc: 0.9984 - breed_acc: 0.9506 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "141/141 [==============================] - 804s 6s/step - loss: 0.1517 - class_loss: 0.0086 - breed_loss: 0.1431 - class_acc: 0.9984 - breed_acc: 0.9508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26a84f24eb8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(\n",
    "    generator = obj, \n",
    "    steps_per_epoch = 141, \n",
    "    epochs = 40,\n",
    "    verbose = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNameId = {'1': [1, 0], '2':[0, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "testObj = ImageSequence(test_csv, test_dir, im_size = 64, batch_size=32, mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 36s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict_generator(generator=testObj, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClass = list()\n",
    "newBreed = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(testObj.imagePathList)):\n",
    "    for j in range(2):\n",
    "        if j==0:\n",
    "            newClass.append((predictions[j][i])/max(predictions[j][i]))\n",
    "        elif j==1:\n",
    "            newBreed.append((predictions[j][i])/max(predictions[j][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClass = np.array(newClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "newBreed = np.array(newBreed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClass = newClass.astype(int)\n",
    "newBreed = newBreed.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = trainData['PNEUMONIA']\n",
    "en = LabelEncoder().fit(m)\n",
    "x = en.transform(m)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SPECTRE\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "n = z.fit_transform(x.reshape(len(x), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClass = z.inverse_transform(newClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClass = newClass + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       ...,\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData['class_name'] = newClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData['breed'] = newBreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData['class_name'] = pandas.to_numeric(testData['class_name'], downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = testData[['id', 'class_name', 'breed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_0966</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_6542</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_2360</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_0763</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_6579</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  class_name  breed\n",
       "0  img_0966           1     13\n",
       "1  img_6542           1     14\n",
       "2  img_2360           2     21\n",
       "3  img_0763           1     12\n",
       "4  img_6579           1     12"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsxWriter = pandas.ExcelWriter('test2.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.to_excel(xlsxWriter, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsxWriter.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
